# Agentic RAG for Medical Guidelines

An intelligent Retrieval-Augmented Generation (RAG) system that uses **agentic AI** to autonomously reason about medical queries, decompose complex questions and iteratively retrieve information from WHO medical documents stored in a PostgreSQL vector database.

## ğŸ¯ What Makes This Agentic?

Unlike traditional RAG systems that perform a single retrieval step, this system implements **true agentic behavior**:

1. **Autonomous Reasoning**: The agent analyzes queries and decides what actions to take
2. **Query Decomposition**: Breaks complex questions into sub-queries automatically
3. **Iterative Retrieval**: Performs multiple searches with refined queries based on results
4. **Self-Correction**: Evaluates search results and adapts strategy if needed
5. **Tool Orchestration**: Dynamically uses retrieval tools based on reasoning

### Agentic Process Flow

```
User Query
    â†“
Agent Reasoning (Step 1)
    â”œâ”€ Analyzes: "What information is needed?"
    â”œâ”€ Decides: "Should I search? What terms?"
    â””â”€ Plans: "Break into sub-queries?"
    â†“
Tool Execution (Step 2)
    â”œâ”€ Search 1: "pneumonia symptoms"
    â”œâ”€ Evaluate results
    â”œâ”€ Search 2: "pneumonia treatment" (if needed)
    â””â”€ Evaluate results
    â†“
Agent Reasoning (Step 3)
    â”œâ”€ "Are results sufficient?"
    â”œâ”€ "Need more specific search?"
    â””â”€ "Can I synthesize an answer?"
    â†“
Final Answer Synthesis
```

The agent can perform up to **10 reasoning steps**, making multiple tool calls and refining its approach autonomously.

## ğŸ—ï¸ Architecture

- **LLM**: GPT-4o-mini (OpenAI) for reasoning and synthesis
- **Embeddings**: text-embedding-3-small (OpenAI)
- **Vector Database**: PostgreSQL with pgvector extension
- **Agent Framework**: LangChain + LangGraph (ReAct pattern)
- **API**: FastAPI REST API

## ğŸ“‹ Prerequisites

- Python 3.8+
- Node.js 18+ and npm (for frontend)
- PostgreSQL 12+ with pgvector extension
- OpenAI API key
- PDF documents to ingest (WHO guidelines)

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone <repository-url>
cd Agentic-RAG
```

### 2. Set Up Environment

Create a `.env` file in the `backend` directory:

```env
OPENAI_API_KEY=your_openai_api_key
DB_CONN_STRING=postgresql://user:password@localhost:5432/dbname

# Optional: where to log MLflow runs
# Local sqlite file at project root (recommended for local dev):
MLFLOW_TRACKING_URI="sqlite:///C:/Users/YourUser/Agentic-RAG/mlflow.db"
# Or point to a remote MLflow server:
# MLFLOW_TRACKING_URI="http://localhost:5000"
```

### 3. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 4. Set Up Database

```sql
-- Create database and schema
CREATE DATABASE your_db_name;
CREATE EXTENSION vector;
CREATE SCHEMA test;
```

### 5. Ingest Documents

Place PDF files in `backend/data/raw_pdfs/` and run:

```bash
python -m app.ingestion.ingest_pdfs
```

### 6. Start the API Server

```bash
uvicorn app.main:app --reload
```

The API will be available at `http://localhost:8000`

### 7. Start the Frontend (Optional)

```bash
cd ../frontend
npm install
npm run dev
```

The frontend will be available at `http://localhost:3000` and connects to the backend API automatically.

## ğŸ“ˆ MLflow Tracking & Evaluation

The backend logs every inference and offline evaluation run to MLflow, so you can inspect behavior, performance, and cost over time.

### MLflow Configuration

- Tracking URI is read from `MLFLOW_TRACKING_URI` in `backend/.env` via `app.config` and `app.mlflow_logger`.
- If `MLFLOW_TRACKING_URI` is not set, the backend falls back to a local SQLite file at the project root: `mlflow.db`.

To view runs:

```bash
cd backend/..  # project root
mlflow ui --backend-store-uri sqlite:///$(pwd)/mlflow.db
```

Or set `MLFLOW_TRACKING_URI` and run `mlflow ui`.

### Inference Metrics (rag_inference)

Each question answered by the agent logs a run to the `rag_inference` experiment with:

- Parameters:
  - `model` (e.g. `gpt-4o-mini`)
  - `temperature`
  - `top_k` (retrieval depth)
  - `query` (user question)
  - `prompt_version` (e.g. `system_v1`)
- Metrics:
  - `latency` â€“ end-to-end response time (seconds)
  - `retrieval_count` â€“ how many retrieval/tool calls were made
  - `avg_chunk_distance` â€“ mean similarity distance of retrieved chunks
  - `answer_length_tokens` â€“ rough word-count proxy for answer length
  - `input_tokens` â€“ prompt tokens (from usage metadata or estimated)
  - `output_tokens` â€“ completion tokens (from usage metadata or estimated)
  - `estimated_cost_usd` â€“ approximate OpenAI cost for the call

This gives you visibility into performance, retrieval behavior, verbosity, and cost per query.

### Retrieval Evaluation (hit_rate@k)

There is a small, fixed evaluation set under `backend/app/evaluation/`:

- `eval_dataset.json` â€“ list of objects with:
  - `question`
  - `expected_doc_filename` (the document that should appear in the retrieved chunks)
- `run_eval.py` â€“ runs all questions against the retriever and checks whether the expected document appears in the topâ€‘`k` results.

Run the evaluation from the `backend` directory:

```bash
cd backend
python -m app.evaluation.run_eval
```

This logs a `rag_retrieval_eval` experiment run with:

- Params:
  - `top_k`
  - `num_questions`
- Metrics:
  - `hit_rate_at_k` â€“ fraction of questions where the expected document was retrieved.

## ğŸ“¡ API Usage

### Ask a Question

```bash
curl -X POST "http://localhost:8000/ask?query=What are the symptoms and treatment for pneumonia?"
```

### Example Queries

- Simple: `"What is pneumonia?"`
- Complex: `"Compare the symptoms, diagnosis and treatment options for pneumonia and malaria"`
- Multi-part: `"What are the WHO guidelines for pneumonia management, including diagnosis and treatment?"`

## ğŸ” How It Works

1. **Document Ingestion**: PDFs are chunked, embedded and stored in pgvector
2. **Query Processing**: Agent receives query and reasons about information needs
3. **Autonomous Search**: Agent decides search strategy and executes multiple retrievals
4. **Synthesis**: Agent combines information from multiple sources into final answer

## ğŸ“ Project Structure

```
Agentic-RAG/
â”œâ”€â”€ backend/              # Python FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py      # FastAPI application
â”‚   â”‚   â”œâ”€â”€ db/          # Database models and connection
â”‚   â”‚   â”œâ”€â”€ ingestion/   # PDF ingestion pipeline
â”‚   â”‚   â””â”€â”€ rag/         # Agentic RAG implementation
â”‚   â”œâ”€â”€ data/raw_pdfs/   # Place PDFs here for ingestion
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ frontend/            # React frontend
    â”œâ”€â”€ src/components/   # React components
    â””â”€â”€ package.json
```

## ğŸ› ï¸ Key Technologies

- **Backend**: FastAPI, LangChain, LangGraph, pgvector, PostgreSQL
- **Frontend**: React, Vite, React Markdown
- **AI**: OpenAI GPT-4o-mini, text-embedding-3-small

## ğŸ“ License

[Add your license here]

## ğŸ¤ Contributing

[Add contribution guidelines if needed]
